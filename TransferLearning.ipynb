{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from Run import RunBuilder as RB\n",
    "from Run import RunManager as RM\n",
    "from DataLoading import UdacityDataset as UD\n",
    "from DataLoading import ConsecutiveBatchSampler as CB\n",
    "\n",
    "from model import TransferLearning as TL\n",
    "\n",
    "%run Visualization.ipynb\n",
    "\n",
    "torch.set_printoptions(linewidth=120) \n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = OrderedDict(\n",
    "    learning_rate = [0.001],\n",
    "    batch_size = [50],\n",
    "    num_workers = [1],\n",
    "    #shuffle = [True,False]\n",
    ")\n",
    "m = RM.RunManager()\n",
    "for run in RB.RunBuilder.get_runs(parameters):\n",
    "    network = TL.TLearning()\n",
    "    network.cuda()\n",
    "    network.to(device)\n",
    "    optimizer = optim.Adam(network.parameters(),lr = run.learning_rate,betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001/run.batch_size, amsgrad=False)\n",
    "# modify the time to visible time format, also use it to check the sequency of pictures is from former to current\n",
    "    udacity_dataset = UD.UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera')\n",
    "    dataset_size = int(len(udacity_dataset))\n",
    "    del udacity_dataset\n",
    "    split_point = int(dataset_size * 0.8)\n",
    "\n",
    "    training_set = UD.UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera',\n",
    "                                     select_range=(0,split_point))\n",
    "\n",
    "    validation_set = UD.UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera',\n",
    "                                     select_range=(split_point,dataset_size))\n",
    "    print(\"size of training set :{}\".format(len(training_set)))\n",
    "    print(\"size of validation set :{}\".format(len(validation_set)))\n",
    "    \n",
    "    training_cbs = CB.ConsecutiveBatchSampler(data_source=training_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=1)\n",
    "    training_loader = DataLoader(training_set, sampler=training_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "\n",
    "    validation_cbs = CB.ConsecutiveBatchSampler(data_source=validation_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=1)\n",
    "    validation_loader = DataLoader(validation_set, sampler=validation_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "\n",
    "    m.begin_run( run,network,[run.batch_size,3,224,224] )\n",
    "    for epoch in range(10):\n",
    "        m.begin_epoch()\n",
    "        for training_sample in tqdm(training_loader):\n",
    "            training_sample['image'] = training_sample['image'].squeeze()\n",
    "            training_sample['image'] = torch.Tensor(resize(training_sample['image'], (run.batch_size,3,224,224),anti_aliasing=True))\n",
    "\n",
    "            param_values = [v for v in training_sample.values()]\n",
    "            image,angle = param_values[0],param_values[3]\n",
    "            image = image.to(device)\n",
    "            prediction = network(image)\n",
    "            prediction = prediction.to(device)\n",
    "            labels = angle.to(device)\n",
    "            del param_values, image, angle\n",
    "            if labels.shape[0]!=prediction.shape[0]:\n",
    "                prediction = prediction[-labels.shape[0],:]\n",
    "            training_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "            optimizer.zero_grad()# zero the gradient that are being held in the Grad attribute of the weights\n",
    "            training_loss_angle.backward() # calculate the gradients\n",
    "            optimizer.step() # finishing calculation on gradient \n",
    "        print(\"Done\")\n",
    "# Calculation on Validation Loss\n",
    "        with torch.no_grad():    \n",
    "            for Validation_sample in tqdm(validation_loader):\n",
    "                Validation_sample['image'] = Validation_sample['image'].squeeze()\n",
    "                Validation_sample['image'] = torch.Tensor(resize(Validation_sample['image'], (run.batch_size,3,224,224),anti_aliasing=True))\n",
    "\n",
    "                param_values = [v for v in Validation_sample.values()]\n",
    "                image,angle = param_values[0],param_values[3]\n",
    "                image = image.to(device)\n",
    "                prediction = network(image)\n",
    "                prediction = prediction.to(device)\n",
    "                labels = angle.to(device)\n",
    "                del param_values, image, angle\n",
    "                if labels.shape[0]!=prediction.shape[0]:\n",
    "                    prediction = prediction[-labels.shape[0],:]\n",
    "                validation_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "                m.track_loss(validation_loss_angle)\n",
    "                m.track_num_correct(prediction,labels) \n",
    "        m.end_epoch(validation_set)\n",
    "        torch.save(network.state_dict(), \"/export/jupyterlab/lintao/TransferLearning/Done/Angle_Adam_MSE_Non5000_Paper_Model/Angle_Adam_MSE_Paper_Model-epoch-{}\".format(epoch))\n",
    "    m.end_run()\n",
    "m.save('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Directly from disk\n",
    "\n",
    "tl_model = TLearning().to(device)\n",
    "tl_model.load_state_dict(torch.load('saved_models/TransferLearning/Angle_Adam_MSE_Paper_Model-epoch-4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cnn(tl_model.ResNet.conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                 root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                 select_camera='center_camera')\n",
    "\n",
    "\n",
    "# Load arbitrary data\n",
    "sample = udacity_dataset[3693]\n",
    "show_sample(sample)\n",
    "input_image = sample['image'].reshape(-1, 3, 480, 640).cuda()\n",
    "\n",
    "cam_extractor_tl = CamExtractorTLModel(tl_model)\n",
    "\n",
    "# Forward pass\n",
    "model_output = tl_model(input_image)\n",
    "\n",
    "# Backward pass\n",
    "tl_model.zero_grad()\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(model_output, sample['angle'].cuda().reshape(1,1))\n",
    "loss.backward()\n",
    "\n",
    "cam_image = cam_extractor_tl.to_image(height=480, width=640) # Use this line to extract CAM image from the model!\n",
    "plt.imshow(cam_image[0, :, :], cmap='jet', alpha=0.5) # this shows CAM as overlay to the original input image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09da5c0f0cd944e8a1b92991cb83ad32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "169fc6dcfbf1453bb3f993e8f820fb75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "  1%",
       "layout": "IPY_MODEL_cf39cd2fed3f4160b56b3648d03a035d",
       "max": 541,
       "style": "IPY_MODEL_cdb7f50858604b57ac7918c821d5fc76",
       "value": 4
      }
     },
     "231c4cfb7acf4255bf00331b651e4d23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8903b5f73cfb4e1d8c1b58a76a110ce8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_169fc6dcfbf1453bb3f993e8f820fb75",
        "IPY_MODEL_97870d47c3244e6b97dd316925455a01"
       ],
       "layout": "IPY_MODEL_a8e1b14a27fd48b587e88a8cd3d24a86"
      }
     },
     "97870d47c3244e6b97dd316925455a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_231c4cfb7acf4255bf00331b651e4d23",
       "style": "IPY_MODEL_09da5c0f0cd944e8a1b92991cb83ad32",
       "value": " 4/541 [00:09&lt;17:54,  2.00s/it]"
      }
     },
     "a8e1b14a27fd48b587e88a8cd3d24a86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cdb7f50858604b57ac7918c821d5fc76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "cf39cd2fed3f4160b56b3648d03a035d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
