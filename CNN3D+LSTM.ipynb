{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from Run import RunBuilder as RB\n",
    "from Run import RunManager as RM\n",
    "from DataLoading import UdacityDataset as UD\n",
    "from DataLoading import ConsecutiveBatchSampler as CB\n",
    "\n",
    "from model import Convolution3D as CNN3D\n",
    "\n",
    "%run Visualization.ipynb\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = OrderedDict(\n",
    "    file = ['3DCNN_Paper'], # used to mark specific files in case that we want to check them on tensorboard\n",
    "    learning_rate = [0.001],\n",
    "    batch_size = [5],\n",
    "    seq_len = [5],\n",
    "    num_workers = [2],\n",
    ")\n",
    "m = RM.RunManager()\n",
    "\n",
    "for run in RB.RunBuilder.get_runs(parameters):\n",
    "    network = CNN3D.Convolution3D().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(network.parameters(),lr = run.learning_rate,betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)\n",
    "\n",
    "    udacity_dataset = UD.UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera')\n",
    "\n",
    "    dataset_size = int(len(udacity_dataset))\n",
    "    del udacity_dataset\n",
    "    split_point = int(dataset_size * 0.8)\n",
    "\n",
    "    training_set = UD.UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera',\n",
    "                                     select_range=(0,split_point))\n",
    "    validation_set = UD.UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera',\n",
    "                                     select_range=(split_point,dataset_size))\n",
    "\n",
    "    training_cbs = CB.ConsecutiveBatchSampler(data_source=training_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=run.seq_len)\n",
    "    training_loader = DataLoader(training_set, sampler=training_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "    \n",
    "    validation_cbs = CB.ConsecutiveBatchSampler(data_source=validation_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=run.seq_len)\n",
    "    validation_loader = DataLoader(validation_set, sampler=validation_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "    \n",
    "    m.begin_run( run,network,[run.batch_size,3,run.seq_len,120,320] )\n",
    "    for epoch in range(10):\n",
    "        m.begin_epoch()\n",
    "# Calculation on Training Loss\n",
    "        for training_sample in tqdm(training_loader, total=int(len(training_set)/run.batch_size/run.seq_len)):\n",
    "            training_sample['image'] = torch.Tensor(resize(training_sample['image'], (run.batch_size,run.seq_len,3,120,320),anti_aliasing=True))\n",
    "            training_sample['image'] = training_sample['image'].permute(0,2,1,3,4)\n",
    "            \n",
    "            param_values = [v for v in training_sample.values()]\n",
    "            image,angle = param_values[0],param_values[3]\n",
    "            image = image.to(device)\n",
    "            prediction = network(image)\n",
    "            prediction = prediction.squeeze().permute(1,0).to(device)\n",
    "            labels = angle.to(device)\n",
    "            del param_values, image, angle\n",
    "            if labels.shape[0]!=prediction.shape[0]:\n",
    "                prediction = prediction[-labels.shape[0],:]\n",
    "            training_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "            optimizer.zero_grad()# zero the gradient that are being held in the Grad attribute of the weights\n",
    "            training_loss_angle.backward() # calculate the gradients\n",
    "            optimizer.step() # finishing calculation on gradient\n",
    "        print(\"Done\")\n",
    "# Calculation on Validation Loss\n",
    "        with torch.no_grad():    \n",
    "            for Validation_sample in tqdm(validation_loader, total=int(len(validation_set)/run.batch_size/run.seq_len)):\n",
    "                Validation_sample['image'] = torch.Tensor(resize(Validation_sample['image'], (run.batch_size,run.seq_len,3,120,320),anti_aliasing=True))\n",
    "                Validation_sample['image'] = Validation_sample['image'].permute(0,2,1,3,4)\n",
    "\n",
    "                param_values = [v for v in Validation_sample.values()]\n",
    "                image,angle = param_values[0],param_values[3]\n",
    "                image = image.to(device)\n",
    "                prediction = network(image)\n",
    "                prediction = prediction.squeeze().permute(1,0).to(device)\n",
    "                labels = angle.to(device)\n",
    "                del param_values, image, angle\n",
    "                if labels.shape[0]!=prediction.shape[0]:\n",
    "                    prediction = prediction[-labels.shape[0],:]\n",
    "                validation_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "                m.track_loss(validation_loss_angle)\n",
    "                m.track_num_correct(prediction,labels) \n",
    "        m.end_epoch(validation_set)\n",
    "        torch.save(network.state_dict(), \"saved_models/CNN3D/epoch-{}\".format(epoch))\n",
    "    m.end_run()\n",
    "m.save('result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model directly from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3d_model = Convolution3D().to(device)\n",
    "cnn3d_model.load_state_dict(torch.load('saved_models/CNN3D/3DCNN_Model-epoch-4'))\n",
    "\n",
    "visualize_cnn(cnn3d_model.Convolution1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_cam_extractor = CamExtractor3DCNN(cnn3d_model)\n",
    "\n",
    "Batch_size = 5\n",
    "Seq_len = 5 \n",
    "\n",
    "\n",
    "udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                 root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                 select_camera='center_camera')\n",
    "\n",
    "# 3D CNN has different input batch size and seq_len, so we define new CBS and Loader\n",
    "\n",
    "cbs_3dcnn = ConsecutiveBatchSampler(data_source=udacity_dataset, batch_size=Batch_size, shuffle=False, drop_last=False, seq_len=Seq_len)\n",
    "loader_3dcnn = DataLoader(udacity_dataset, sampler=cbs_3dcnn, collate_fn=(lambda x: x[0]))\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "for i, testing_sample in enumerate(loader_3dcnn):\n",
    "    testing_sample['image'] = torch.Tensor(resize(testing_sample['image'], (Batch_size,Seq_len,3,120,320),anti_aliasing=True))\n",
    "    testing_sample['image'] = testing_sample['image'].permute(0,2,1,3,4)\n",
    "    image = testing_sample['image'].to(device)\n",
    "    prediction = cnn3d_model(image).squeeze()\n",
    "    target = testing_sample['angle'].to(device)\n",
    "    loss = mse_loss(prediction, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    plt.figure()\n",
    "    cam_image = cnn_cam_extractor.to_image(width=320, height=120) # Use this line to extract CAM image from the model!\n",
    "    plt.imshow(testing_sample['image'][0, :, -1, :, :].permute(1, 2, 0))\n",
    "    plt.imshow(cam_image[0, -1, :, :], cmap='jet', alpha=0.5)\n",
    "    \n",
    "    if i==3: # show only a few for testing\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "011ebb92fbd24a819de2f622a59912ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_607feafdf31142aea308be66f666b504",
       "style": "IPY_MODEL_42ad982373e04ad8be4d990fba88c551",
       "value": " 6/1081 [00:07&lt;20:48,  1.16s/it]"
      }
     },
     "42ad982373e04ad8be4d990fba88c551": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "607feafdf31142aea308be66f666b504": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7b00bc7d78d841de990c6833c7066e8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d569713a50a248f0800bbcfc12eab14f",
        "IPY_MODEL_011ebb92fbd24a819de2f622a59912ad"
       ],
       "layout": "IPY_MODEL_b7d0898a52a9461aa33c3720aec562b4"
      }
     },
     "85e3c0c5fc464ebb9b00d0e7c30eabcb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b7d0898a52a9461aa33c3720aec562b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d569713a50a248f0800bbcfc12eab14f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "  1%",
       "layout": "IPY_MODEL_85e3c0c5fc464ebb9b00d0e7c30eabcb",
       "max": 1081,
       "style": "IPY_MODEL_f1d240b7ad4f475d950d95b033d66e85",
       "value": 6
      }
     },
     "f1d240b7ad4f475d950d95b033d66e85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
